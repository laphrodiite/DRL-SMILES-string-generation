{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0504dcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 molecules. Example:\n",
      "                                                   smiles\n",
      "206374               C/[NH+]=C(/NCCCc1cccc(Br)c1)NC1CC1\\n\n",
      "130740        Cc1cccc(-n2nnnc2S[C@@H](C)C(=O)N2CCCC2)c1\\n\n",
      "175485           CCc1nncn1CCNC(=O)C(=O)Nc1ccc(C(C)C)cc1\\n\n",
      "59193   CC(=O)NCc1ccc(S(=O)(=O)N2CCC[C@H]2c2ccc(C)c(C)...\n",
      "211224                  c1ccc(OCc2nnc(SCc3cccnc3)o2)cc1\\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating SMILES: 100%|██████████| 10000/10000 [00:02<00:00, 3658.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid SMILES: 10000/10000\n",
      "Vocabulary size: 36\n",
      "Example tokens: ['\\n', '#', '(', ')', '+', '-', '/', '1', '2', '3']...\n",
      "Batch shape: torch.Size([64, 99]), torch.Size([64, 99])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# Load from local file\n",
    "local_file = \"250k_rndm_zinc_drugs_clean_3.csv\"\n",
    "zinc_df = pd.read_csv(local_file, sep=',', usecols=['smiles']).sample(10000)  # Only load 'smiles' column\n",
    "zinc_df.to_csv(\"zinc_sample_10k.csv\", index=False)\n",
    "print(f\"Loaded {len(zinc_df)} molecules. Example:\\n{zinc_df.head()}\")\n",
    "\n",
    "# Validate and clean-up SMILES\n",
    "def validate_smiles(smiles_list):\n",
    "    valid_smiles = []\n",
    "    for smi in tqdm(smiles_list, desc=\"Validating SMILES\"):\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:  # Only keep valid molecules\n",
    "            valid_smiles.append(smi)\n",
    "    return valid_smiles\n",
    "\n",
    "valid_smiles = validate_smiles(zinc_df['smiles'].tolist())\n",
    "print(f\"Valid SMILES: {len(valid_smiles)}/{len(zinc_df)}\")\n",
    "\n",
    "# Build vocabulary with tokenization\n",
    "tokens = set()\n",
    "for smi in valid_smiles:\n",
    "    tokens.update(list(smi))\n",
    "vocab = sorted(tokens) + [\"<PAD>\", \"<START>\", \"<END>\"]\n",
    "token_to_idx = {t:i for i,t in enumerate(vocab)}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Example tokens: {list(vocab)[:10]}...\")\n",
    "\n",
    "# Create dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SmilesDataset(Dataset):\n",
    "    def __init__(self, smiles, token_to_idx, max_len=100):\n",
    "        self.smiles = smiles\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smile = self.smiles[idx]\n",
    "        # Add start/end tokens and pad\n",
    "        tokens = [\"<START>\"] + list(smile) + [\"<END>\"]\n",
    "        tokens = tokens[:self.max_len] + [\"<PAD>\"] * (self.max_len - len(tokens))\n",
    "        indices = [self.token_to_idx[t] for t in tokens]\n",
    "        return torch.tensor(indices[:-1]), torch.tensor(indices[1:])  # (input, target)\n",
    "\n",
    "# Initialize DataLoader\n",
    "dataset = SmilesDataset(valid_smiles, token_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Verify one batch\n",
    "for x, y in dataloader:\n",
    "    print(f\"Batch shape: {x.shape}, {y.shape}\")  # Should be (64, max_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac48779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([64, 99]), torch.Size([64, 99])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SmilesDataset(Dataset):\n",
    "    def __init__(self, smiles, token_to_idx, max_len=100):\n",
    "        self.smiles = smiles\n",
    "        self.token_to_idx = token_to_idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        smile = self.smiles[idx]\n",
    "        # Add start/end tokens and pad\n",
    "        tokens = [\"<START>\"] + list(smile) + [\"<END>\"]\n",
    "        tokens = tokens[:self.max_len] + [\"<PAD>\"] * (self.max_len - len(tokens))\n",
    "        indices = [self.token_to_idx[t] for t in tokens]\n",
    "        return torch.tensor(indices[:-1]), torch.tensor(indices[1:])  # (input, target)\n",
    "\n",
    "# Initialize DataLoader\n",
    "dataset = SmilesDataset(valid_smiles, token_to_idx)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Verify one batch\n",
    "for x, y in dataloader:\n",
    "    print(f\"Batch shape: {x.shape}, {y.shape}\")  # Should be (64, max_len)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa294d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0032\n",
      "Epoch 2, Loss: 0.8726\n",
      "Epoch 3, Loss: 0.7316\n",
      "Epoch 4, Loss: 0.8609\n",
      "Epoch 5, Loss: 0.7826\n",
      "Epoch 6, Loss: 0.6721\n",
      "Epoch 7, Loss: 0.7048\n",
      "Epoch 8, Loss: 0.6463\n",
      "Epoch 9, Loss: 0.7090\n",
      "Epoch 10, Loss: 0.6082\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SmilesLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize\n",
    "model = SmilesLSTM(len(vocab))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token_to_idx[\"<PAD>\"])\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Start with 10 epochs\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs.reshape(-1, len(vocab)), y.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "214669d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_smile(model, token_to_idx, vocab, max_len=100):\n",
    "    \"\"\"Generate a SMILES string autoregressively.\"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    tokens = [\"<START>\"]\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for _ in range(max_len):\n",
    "            # Convert current tokens to tensor\n",
    "            input = torch.tensor([token_to_idx[t] for t in tokens]).unsqueeze(0)  # Shape: (1, seq_len)\n",
    "            \n",
    "            # Get model prediction\n",
    "            output = model(input)  # Shape: (1, seq_len, vocab_size)\n",
    "            \n",
    "            # Take the last predicted token\n",
    "            last_output = output[:, -1, :]  # Shape: (1, vocab_size)\n",
    "            next_token_idx = last_output.argmax(-1).item()  # Scalar index\n",
    "            next_token = vocab[next_token_idx]\n",
    "            \n",
    "            if next_token == \"<END>\":\n",
    "                break\n",
    "            tokens.append(next_token)\n",
    "    \n",
    "    return \"\".join(tokens[1:])  # Remove <START>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c326ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validity rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "def is_valid(smile):\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    print(mol)\n",
    "    return mol is not None and len(smile) > 0  # Explicit length check\n",
    "\n",
    "valid = 0\n",
    "for _ in range(10):\n",
    "    valid += int(is_valid(generate_smile(model, token_to_idx, vocab)))\n",
    "print(f\"Validity rate: {valid/10}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
